# 贝叶斯滤波框架综述

## 从一般性的估计问题谈起——频率学派与贝叶斯学派

贝叶斯滤波是采用贝叶斯原理来处理滤波问题的滤波方法。为了透彻理解贝叶斯滤波，我们首先从统计学中的**估计**问题谈起。对于某个总体，若其概率分布已知则称为总体已知，因此***总体是定义在概率空间上的概率测度***。我们可以对总体做一些独立实验，从而得到一系列的**独立同分布（i.i.d）**的样本$X_i$，它是一个***随机变量***，其取值为$x_i\in\mathbb{R}^{d_x}$（可以拓展到复数域），是一个***变量***。估计的关键在于通过有限的样本获得我们期望的**估计量**。估计量是一种特殊的**统计量**，统计量的定义为样本的一个已知函数$T(X)$，当样本取值确定时统计量的取值也随之确定，因此统计量是一个随机变量，服从一定的分布，虽然这个分布可能并不一定存在解析表达式。常用的统计量有：**样本平均值、样本方差和次序统计量**。包含一定目的的统计量为估计量，一般假设带估计参数为$\theta$，那么估计量为函数$\hat\theta(X)$，同样估计量也是一个随机变量。

### 频率学派参数估计

对于待估参数$\theta$，经典参数估计理论认为它是一个固定的未知量，是一个变量，不服从任何分布假设。用柏拉图的*洞穴比喻*来说明，待估参数就是真实的物体，而我们看到的（做的独立实验）是投影在洞穴壁上的影子，我们需要观察影子来估计真实的待估参数。

与估计量相关的几个概念定义如下：

- 误差：$e(x)=\hat\theta(x)-\theta$；
- 均方误差（MSE）：$MSE(\hat\theta)=\mathbb{E}[(\hat\theta(X)-\theta)^2]$，MSE描述了统计量和真实值之间的误差；
- 偏差：$B(\hat\theta)=\mathbb{E}(\hat\theta)-\theta$，$B(\hat\theta)=0$的估计量为无偏估计量
- 样本偏差：$d(x)=\hat\theta(x)-\mathbb{E}[\hat\theta]$，样本偏差是给定样本的情况下的一个值；
- 方差：$\mathrm{var}(\hat\theta)=\mathbb{E}[(\hat\theta-\mathbb{E}[\hat\theta])^2]$，方差描述了估计量的性能；

其中估计量的均方误差、偏差和方差均为描述估计量性能的指标。他们之间的的关系为
$$
MSE(\hat\theta)=\mathrm{var}(\hat\theta)+(B(\hat\theta))^2\tag{1}
$$
在传统的估计框架下，由于$\theta$是固定的未知量，**因此MSE往往依赖于$\theta$，从而导致无法调整估计量来使得最小进而获得最小MSE估计量**。因此往往限定估计量是无偏的，再进一步寻找方差最小的估计量，称为**最小方差无偏估计量（MVU）**，由（1）可知方差最小的无偏估计量在无偏估计量中也是MSE最小的。

尽管我们期望能够得到MVU估计量，但是它往往很难求得。CRLB给出了无偏估计量的下限，达到CRLB的估计量即为MVU估计量。可以藉由**充分统计量**来确定MVU，具体方法是通过**Neyman-Fisher因子分解定理**。在无法确定MVU估计量的情况下还存在其他的方法如：***最大似然法、矩估计法和BLUE等***。

### 贝叶斯估计

而在贝叶斯统计中，待估参数是一种「信念」。而我们提前拥有关于这个信念的一部分知识，称为**先验**。而对于事件的观测则会影响我们对该事物的知识（**似然**），影响后的知识为**后验**。先验知识和后验结果都以概率的形式体现出来：先验分布$p(\theta)$，似然$p(x|\theta)$和后验分布$p(\theta|x)$。而这三者通过**贝叶斯公式**关联起来：
$$
p(\theta|x)=\frac{p(x|\theta)p(\theta)}{\int p(x|\theta)p(\theta)d\theta}\tag2
$$

在贝叶斯框架下对估计量性能进行衡量，最先想到的仍然是使用均方误MSE，与传统估计不同的是待估参数也是随机变量，因此有
$$
BMSE(\hat\theta)=\mathbb{E}[(\theta-\hat\theta(X))^2]=\iint(\theta-\hat\theta)^2p(x,\theta)dxd\theta\tag{3}
$$
应当注意定义中的$\theta$和$\hat\theta$的位置与经典框架下的区别。此时的贝叶斯MSE（BMSE）已经与待估参数无关，因此可以调整$\hat\theta$来使得BMSE最小从而获得**最小均方误差（MMSE）估计量**，对（3）关于$\hat\theta$求导并令其等于零可以***得到MMSE估计量为后验期望***：
$$
\hat\theta_{MMSE}=\mathbb{E}[\theta|x]=\int\theta p(\theta|x)d\theta
$$
更一般地，令误差$e=\theta-\hat\theta$，定义关于误差的**代价函数**$C(e)$，代价函数是一个确定性的函数，定义平均代价为**贝叶斯风险**$\mathcal{R}=\mathbb{E}[C(e)]$。令不同的代价函数（准则）最小可以得到不同的估计量，总结如下表

| 代价函数$C(e)$   | 估计量$\hat\theta$                  |
| ---------------- | ----------------------------------- |
| $e^2$ 二次型误差 | $\mathbb{E}[\theta|x]$，后验PDF期望 |
| $|e|$ 绝对值误差 | 后验PDF中值                         |
| 成功-失败误差    | 后验PDF众数（最大值）               |

贝叶斯估计最令人难以接受的地方便是计算后验分布。尤其是在确定MMSE估计量时，首先需要计算（2）中分母上的积分（在实际问题中可能是多维积分）。然后还需要计算复杂的期望中的积分。不过**Monte-Carlo方法**为积分的数值求解提供了极大的便利，我们将在本文后续介绍Monte-Carlo方法。

### 小结

经典的参数估计理论把参数作为未知的确定量即变量，这往往导致估计量的MSE自身与待估参数有关，从而无法求出MMSE估计量，取而代之的是我们希望可以求得MVU估计量，它是无偏估计量中最优的。**但是MSE准则下的最优估计量并不一定是无偏的**。贝叶斯估计量把待估参数看作是随机变量，在求估计量的MSE时待估参数也同时被求期望，因此得到的MSE与参数无关，可以求得MMSE估计量，经过简单的推导可知MMSE估计量就是后验PDF的数学期望。

## 最优线性滤波器——LMMSE滤波器

在这一部分我们进入贝叶斯估计在信号处理领域的应用，首先考虑最一般性的滤波问题：通过输入数据$X$来推测期望响应$Y$。一般$X$代表了当前时刻之前的$M$个输入信号$[x_{n-M+1},...,x_n]$，$Y$为被估计的参数$y_n$。我们**假定$y$和$x $均是零均值的**。应用贝叶斯公式（2）可以求得后验分布为
$$
p(y|\boldsymbol{x})=\frac{p(\boldsymbol{x}|y)p(y)}{p(\boldsymbol{x})}
$$
上述公式中的任意一项都很难求得，往往只能使用统计手段估计出来，这需要大量的数据量。一个简单的解决方案是假定输入信号之间不存在相关性，则似然函数可以写作$p(\boldsymbol{x}|y)=\prod_i p(x_i|y)$，这种方法在在**朴素贝叶斯分类器**中使用。

### LMMSE滤波器推导

回想DSP中的数字滤波器，两种最经典的滤波器都是线性的：IIR和FIR滤波器。现在我们约束对$y$的估计也是线性的：$\hat y=\boldsymbol{c}^H\boldsymbol{x}$，再从一定准则来推导线性系数$\boldsymbol{c}$。此时的估计器的MSE为系数$\boldsymbol{c}$的函数：
$$
\begin{align}
\mathbb{E}[|y-\hat y|^2]&=\mathbb{E}[(y-\hat y)^H(y-\hat y)]\\
&=\mathbb{E}{[|y|^2]}-\boldsymbol{c}^H\mathbb{E}[\boldsymbol{x}y^*]-\mathbb{E}[y\boldsymbol{x}^H]\boldsymbol{c}+\boldsymbol{c}^H\mathbb{E}[\boldsymbol{xx}^H]\boldsymbol{c}
\end{align}
$$
令上式中第一项为$P_y$代表了期望响应的功率，第二项中的期望为$\bold{d}$代表了输入数据和期望响应中间的互相关，最后一项中的期望为输入信号的协方差矩阵。那么上式为系数的确定函数称为**误差性能表面**
$$
P(\boldsymbol{c})=P_y-\boldsymbol{c}^H\bold{d}-\bold{d}^H\boldsymbol{c}+\boldsymbol{c}^H\bold{R}\boldsymbol{c}\tag4
$$
协方差矩阵一定是半正定矩阵，实际数据的协方差矩阵通常都是正定矩阵。约束$\bold{R}$是正定矩阵，那么（4）可以转换为
$$
P(\boldsymbol{c})=P_y-\bold{d}^H\bold{R}^{-1}\bold{d}+(\bold{R}\boldsymbol{c}-\bold{d})^H\bold{R}^{-1}(\bold{R}\boldsymbol{c}-\bold{d})
$$
上式中只有最后一项与$\boldsymbol{c}$有关，因此当系数满足**正则方程**
$$
\begin{cases}
\bold{R}\boldsymbol{c}-\bold{d}=0
\\\bold{R}\succ0
\end{cases}
$$
此时的线性滤波器的MSE最小，这种滤波器称为LMMSE滤波器。当输入和期望响应均为平稳过程时，它也被称为**维纳滤波器**。

### 小结

LMMSE滤波器对观测值和待估参数的分布没有作出任何假设，它约束估计量为观测的线性组合。所需要的**先验知识**仅仅为观测和待估参数的**前两阶矩**，因此是一种**非参数方法**。在实际应用中设计LMMSE滤波器时，为了能够获得数据的二阶矩，我们可以通过大量的数据进行统计来得到**二阶矩的估计量**，进而应用LMMSE滤波器；或者我们可以假定系统模型和分布来计算二阶矩，但是这种方法就失去了LMMSE的意义。另外的解决方案是使用**LSE滤波器**。

## 序贯贝叶斯滤波

在上一部分的LMMSE滤波中，我们没有对观测和待估参数进行建模，这省去了贝叶斯滤波中繁琐的积分计算而以简洁的正则方程进行代替。这种方法在线性条件下是最优的，但是在非线性情况下则性能很差。考虑极端情况，假定观测和待估参数是**线性不相关**的，则有$\bold{d}=0$，**此时的LMMSE滤波器甚至不存在**。

而且很多时候观测值和待估参数的关系明显可以使用某种模型表达出来，即可以表达为$y=g(x,d)$。其中$x$即为我们需要估计的参数，而$y$通常为我们需要**测量**的参数，而式中的$d$代表了这个过程中的噪声。如果假定$x$是固定的未知量，那么这个测量方程定义了一个似然函数$L(y;x)$，可以由最大似然估计得到。或者给出$g$的更加确切的形式来求得含参$y$的pdf从而使用一般的经典参数估计方法。实际上，待估参数$x$往往具有一定的物理意义，比如说我们需要定位目标的位置，可以认为目标在匀速、加速，而$x$代表了某种状态。这种状态可以使用一个函数来描述，在离散情况下，状态可以定义为$x_{n+1}=f_n(x_n, v_n)$。该状态方程为$x$的估计提供了一个先验信息，我们称为**状态方程**。

通过上述讨论可以看到可以通过一个测量方程和一个状态方程把观测和物理过程结合起来。我们称其为**状态空间模型（SSM）**（应当注意与上一部分在符号上的差异，在推导LMMSE滤波器时我们使用$x$作为观测$y$作为待估参数，而在这一部分状态向量$x$作为待估参数而$y$作为观测）：
$$
\begin{cases}
x_{n+1}=f_n(x_n,v_n)\\
y_n=g_n(x_n, d_n)\tag5
\end{cases}
$$
结合上面的讨论我们看到状态方程提供了贝叶斯估计中需要的**先验知识**，它通过上一时刻的状态给出这一时刻的状态的先验分布。状态方程定义了**状态转移概率**$p(x_{n+1}|x_n)$。而测量方程提供了**似然**$p(y_n|x_n)$。而我们希望通过滤波得到的是$p(x_n|\bold{Y}_n)$（有时是全部状态的后验分布$p(\bold{X}_n|\bold{Y}_n）$)，其中$\bold{Y}_n=y_{1:n} $。接下来我们将通过状态空间模型把贝叶斯滤波推广为序贯滤波的形式。

假定初始分布$p(x_0)$、噪声的$v_n$和$d_n$的概率分布是已知的。我们已经得到了上一时刻的后验分布$p(x_{n-1}|\bold{Y}_{n-1})$，那么有
$$
\begin{align}
p(x_n|\bold{Y}_n)&=\frac{p(\bold{Y}_n|x_n)p(x_n)}{p(\bold{Y}_n)}
\\&=\frac{p(y_n|\bold{Y}_{n-1},x_n)p(\bold{Y}_{n-1}|x_n)p(x_n)}{p(y_n|\bold{Y}_{n-1})p(\bold{Y}_{{n-1}})}
\\&=\frac{p(y_n|\bold{Y}_{n-1},x_n)p(x_n|\bold{Y}_{n-1})p(\bold{Y}_{n-1})p(x_n)}{p(y_n|\bold{Y}_{n-1})p(\bold{Y}_{n-1})p(x_n)}
\\&=\frac{p(y_n|x_n)p(x_n|\bold{Y}_{n-1})}{p(y_n|\bold{Y}_{n-1})}\tag6
 \end{align}
$$
其中分子第一项即为似然，第二项为先验，表示了给定之前的观测推理出来的现在的知识：
$$
p(x_n|\bold{Y}_{n-1})=\int p(x_n|x_{n-1})p(x_{n-1}|\bold{Y}_{n-1})dx_{n-1}	\tag7
$$
一般文献中称计算先验（7）为**预测**，表示根据过去的观测预测新的状态；称共识（6）为**更新**，表示观测数据对预测结果的校正。于是序贯贝叶斯就是通过不断地**预测--观测--更新估计**来得到待估参数的后验分布的。

对于后验分布$p(\bold{X}_n|\bold{Y}_n)$的后验分布有递推公式：
$$
p(\bold X_n|\bold Y_n)=p(\bold X_{n-1}|\bold Y _{n-1})\frac{p(x_n|x_{n-1})p(y_n|x_n)}{p(y_n|\bold Y_{n-1})}
$$
很容易出可以验证：$p(x_n|\bold Y_n)=\int p(\bold X_n|\bold Y_n)d\bold{X}_{n-1}$。

但是很遗憾的是我们以上过程往往并没有闭式解，因为其中的任何概率分布都可能非常难以计算。但是在一类特殊条件下闭式解是存在的，此时的贝叶斯滤波器称为**卡尔曼滤波（KF）**。

### 卡尔曼滤波

 卡尔曼滤波考虑最一般的SSM情况：线性模型且噪声均为加性高斯白噪声，此时的SSM为
$$
\begin{cases}
x_{n+1}=\bold{F}_nx_n+d_n
\\y_n=\bold{G}_nx_n+v_n
\end{cases}
$$
其中，$x\in\R^{d_x}$，$y\in\R^{d_y}$，$d_n\sim\mathcal{N}(0,\bold{\Sigma}_d)$，$v_n\sim\mathcal{N}(0,\bold{\Sigma}_v)$，各矩阵的维度均满足矩阵相乘的计算要求，且满足以下假设：

- 噪声为白噪声：$\mathbb{E}[d_nd_m]=\bold{\Sigma}_d\delta_{mn}$，$\mathbb{E}[v_nv_m]=\bold{\Sigma}_v\delta_{mn}$；
- 状态和噪声独立：$\mathbb{E}[{x_nd_m^T}]=0，n\leq m$，$\mathbb{E}[x_nv_m^T]=0$；
- 测量噪声和模型噪声独立：$\mathbb{E}[d_nv_m^T]=0$；
- 初始分布为高斯分布：$p(x_0)\sim\mathcal{N}$。

由此可知似然函数满足$p(y_n|x_n)\sim\mathcal{N}(\bold{G}_nx_n,\bold{\Sigma}_d)$。因为状态变量的初始分布为高斯分布，噪声也为高斯分布，故所有的先验分布均为高斯的，又似然函数是高斯分布，高斯分布的共轭先验也是高斯分布，故后验分布$p(x_{n-1}|\bold{Y}_{n-1})$仍为高斯分布，其均值即为估计结果，记为$\hat x_{n-1|n-1}$（简记为$\hat x_{n-1}$），设其协方差矩阵为$\bold{P}_{n-1}$，那么有$\bold{P}_{n-1}=\mathrm{cov}[e_{n-1}]$，其中$e_{n-1}=x_{n-1}-\hat x_{n-1}$为上一时刻的**后验误差向量**。

经过预测公式（7）的演化得到后验分布为$p(x_{n}|\bold{Y}_{n-1})$，因为前一时刻的后验分布和状态转移概率分布均为高斯分布，故此分布仍为高斯分布。可以证明其期望
$$
\hat x_{n|n-1}=\bold{F}_{n-1}\hat x_{n-1}\tag8
$$
其方差为$\bold{P}_{n|n-1}=\mathbb{E}[(x_n-\hat x_{n|n-1})^2]=\mathbb{E}[e^2_{n|n-1}]$，它代表了**先验误差的协方差矩阵**。根据先验误差和上一时刻的后验误差的关系$e_{n|n-1}=\bold{F}_{n-1}e_{n-1}+d_{n-1}$，可以得到先验误差的递推公式：
$$
\bold{P}_{n|n-1}=\bold{F}_{n-1}\bold{P}_{n-1}\bold{F}_{n-1}^T+\bold{\Sigma}_d\tag{9}
$$
将后验分布带入更新公式（6），其中似然$p(y_n|x_n)\sim\mathcal{N}(\bold{G}_nx_n,\bold{\Sigma}_d)$。两个高斯分布相乘再除以归一化因子仍为高斯分布，此时分布的指数内部分为：
$$
-\frac{1}{2}(\|y_n-\bold{G}_nx_n\|^2_{\bold{\Sigma}_v^{-1}}+\|x_n-\hat x_{n|n-1}\|^2_{\bold{P}_{n|n-1}})
$$
为二次型的形式，为了求其中心值可以对其求偏导数等于零或者对二次型进行化简，最后得到的结果为：
$$
\hat x_n=(\mathbf{G}_{n}^{T} \bold\Sigma_{v}^{-1} \mathbf{G}_{n}+\mathbf{P}_{n| n-1}^{-1})^{-1}(\mathbf{P}_{n| n-1}^{-1} \hat{{x}}_{n \mid n-1}+\mathbf{G}_{n}^{T} \bold\Sigma_{{v}}^{-1} {y}_{n})
$$
使用**矩阵求逆引理**得到：
$$
\hat x_n=\hat x_{n|n-1}+\bold{K}_n(y_n-\bold{G}_n\hat x_{n|n-1})\tag{10}
$$
其中$\mathbf{K}_{n}$为**卡尔曼增益矩阵**：
$$
\mathbf{K}_{n}=\mathbf{P}_{n|n-1} \mathbf{G}_{n}^{T}\left(\mathbf{G}_{n} \mathbf{P}_{n|n-1} \mathbf{G}_{n}^{T}+\Sigma_{v}\right)^{-1}\tag{11}
$$
此时（10）的物理意义是明显的，公式右侧第一项为先验知识的预测结果；而第二项括号中的部分表示了测量数据和先验预测之间的差异称为**新息（innovation）**，代表了新的观测数据对模型的修正。其中卡尔曼增益矩阵起到了权衡先验和观测的作用。考虑到得到状态的预测时有观测值的预测：$\hat y=\bold G_n\hat x_{n|n-1} + v_n$，那么预测测量协方差矩阵为：
$$
\bold R_{\hat y}=\mathbb{E}[yy^T]=\bold G_n\bold P_{n|n-1}\bold G_n^T + \Sigma_v
$$
因此卡尔曼增益可以进一步写作：
$$
\bold K_n=\bold P_{n|n-1}\bold G_n^T \bold R_{\hat y}
$$
把（10）带入后验误差公式$e_n=x_n-\hat x_n$并求其协方差矩阵可以得到后验协方差矩阵的递推公式：
$$
\mathbf{P}_{n}=(\bold{I}-\bold{K}_n\bold{G}_n)\mathbf{P}_{n|n-1}\tag{12}
$$
至此到下一时刻滤波所需要的全部信息已经被递推地表达了出来。公式（8）到公式（12）构成了卡尔曼滤波的全部内容，其中（8）和（9）对应贝叶斯滤波的预测阶段，公式（9）到（12）为贝叶斯滤波的更新阶段。

从序贯贝叶斯滤波的推导中我们可以看到序贯贝叶斯滤波是一个不断更新概率分布的过程：由之前的先验分布结合状态模型得到先验分布，再结合测量数据得到新的后验分布。卡尔曼滤波限制状态方程和测量方程都是线性加性高斯白噪声的，***这保证了不停演化的分布均为高斯分布***，因为**高斯分布完全由均值和方差决定**，因此在贝叶斯滤波中只需要均值和期望在序贯滤波的过ß程中进行更新即可。卡尔曼滤波的PDF演化过程如下：从前一时刻的后验分布$p(x_{n-1}|\bold{Y}_{n-1})\sim\mathcal{N}(\hat x_{n-1},\bold{P}_{n-1})$，线性加性高斯的状态模型保证了预测的先验分布也是高斯的$p(x_n|\bold{Y}_{n-1})\sim\mathcal{N}(\hat x_{n|n-1},\bold{P}_{n|n-1})$，线性加性高斯的测量模型保证了似然是高斯的，因此后验分布也是高斯的$p(x_n|\bold{Y}_n)\sim\mathcal{N}(\hat x_n, \bold{P}_n)$。

卡尔曼滤波取了后验分布的期望作为估计值，因此它是MSE准则下最优的线性估计器（实际上高斯分布的期望中值和众数是一致的，因此他也是MAP准则下最优的）。高斯分布的优良性质保证了其解析的递推公式存在。但是很多情况下，卡尔曼滤波假设的SSM过于理想，因此很多新的贝叶斯滤波方法在**非线性非高斯**的假设下被提出。

## 序贯蒙特卡洛滤波

蒙特卡洛滤波是一种特殊的贝叶斯滤波，他应用蒙特卡洛方法来计算不好计算的积分问题。简单的来说，蒙特卡洛方法就是使用随机采样来模拟某个分布，再通过这个分布来估计与此分布相关的量如均值方差等。

假设存在一个我们可以采样但是不知道其具体分布的概率分布$p(x)$，我们可以独立采样$N_s$个随机样本$X^i\sim p(x)$。当采样点足够多时，根据**大数定律**该PDF就可以由这些采样点决定：
$$
\hat p(x) = \frac{1}{N_s}\sum_{i=1}^{N_s}\delta(x-X^i)

\tag{11}
$$
虽然在每个采样数据处的值均为$1/N_s$，但是其**邻域内的样本的疏密足以反映概率密度的大小**。以一维标准正态分布为例，按照分布采样出500个样本点，如图所示。在图中标记出了所有样本点的位置（黄色竖线），标准正态分布的PDF以及以0.25位区间长度的统计直方图。可以看到采样的样本点可以很好的近似PDF，当样本点的数量很大时，近似效果更好。

![1Dgaussian](/Users/liushuduo/OneDrive/My_Documents/Blogs/1Dgaussian.jpg)

该分布对于任意函数的期望于是可以得到：
$$
I^{MC}(\varphi) = \frac{1}{N_s}\sum_{i=1}^{N_s}\varphi(X^i)
$$
而且蒙特卡洛估计量是**无偏估计量**：
$$
\mathbb{E}[I^{MC}] =\frac{1}{N_s}\sum_{i=1}^{N_s}\mathbb{E}[\varphi(X^i)]=\mathbb{E}[\varphi]
$$
蒙特卡洛估计量的方差为：
$$
\mathrm{var}[I^{MC}]=\frac{1}{N_s}\left(\int\varphi^2(x)p(x)dx-I^2(\varphi)\right)
$$
可以看到蒙特卡洛估计的方差随着样本数量的增加而降低且与$x$的维度无关。但是一个关键的问题是**我们经常并不能直接从某个分布中$p(x)$采样**。针对这个问题学者提出了很多蒙特卡洛采样方法，常见的有*重要性采样（IS）、拒绝采样（RS）、序贯重要性采样（SIS）、重要性重采样（SIR）*，此外还有一些基于马尔可夫模特卡洛（MCMC）的采样方法。这里主要说明重要性采样方法。

首先考虑关于分布$p(x)$对某个函数$\varphi(x)$求期望：$\mathbb{E}[\varphi] = \int\varphi(x)p(x)dx$。我们对这个公式做一定的变换可以得到：
$$
\mathbb{E}[\varphi] = \int\varphi(x)\frac{p(x)}{q(x)}q(x)dx
$$
于是上式又可以看作是函数关于$q(x)$求数学期望，而且在每个概率点上都乘上了一个权重因子：$w(x)=p(x)/q(x)$。于是对服从$q(x)$分布的数据进行采样再乘以权重因子就可以求得关于分布$p(x)$对于所有函数的期望。从这里我们可以看到：要求出权重$w$的值需要同时知道$p$和$q$的逐点取值。**而且$q$的定义域非零部分（支撑集）要包含$p$的。**我们称便于采样的分布$q$为**建议分布**，对于任何概率分布$p$假设可以表示成未归一化的形式：
$$
p(x)=\frac{\gamma(x)}{Z}=\frac{w(x)q(x)}{Z}
$$
其中$w(x)=\frac{\gamma(x)}{q(x)}$为（未归一）权重函数。$Z=\int w(x)q(x)dx$为归一化系数。现在从简易分布$q$中采出$N_s$个样本$X^i$，那么归一化系数$Z$的估计根据之前的讨论即为：
$$
\hat Z=\frac{1}{N_s}\sum_{i=1}^{N_s}w(X^i)
$$
根据蒙特卡洛PDF估计公式可以得到：
$$
\hat p(x)=\frac{1}{N_s}\sum_{i=1}^{N_s}\frac{w(X^i)}{\hat Z}\delta_{X^i}(x)=\sum_{i=1}^{N_s}W^i\delta_{X^i}(x)
$$
 其中$W$为归一化的权重，公式为：
$$
W^i=\frac{w(X^i)}{\sum_{j=1}^{N_s}w(X^i)}
$$
此时使用样本来表示PDF的估计时，除了采样点的疏密程度以外，还有用来表示PDF大小的权值。这里以二维的高斯分布来逼近均匀分布作为示例。点的大小表示权重的大小。左图为均匀分布的范围和从高斯分布中采样得到的数据，右图为乘以权重后的采样点。可以看到越靠近中心的点的权重越小，而越靠近边缘的点权重越大。

![2Dcompare](/Users/liushuduo/OneDrive/My_Documents/Blogs/2Dcompare.jpg)

建议分布$q$的选取对于IS来说至关重要。当$p$和$q$成正比时此方法的方差最小。当$p$和$q$的PDF的模式差别很大时，IS的性能很差，考虑极端情况：使用均值在-100处的标准正态分布区逼近均值在100处的标准正态分布，从第一个PDF中产生的采样点几乎不会在第二个PDF的高概率处出现，此时的IS性能非常差。

### 粒子滤波

粒子滤波是基于模特卡洛原理的贝叶斯序贯滤波方法，它使用一系列的粒子$\{x_{0:n}^i,w_n^i\}_{i=1}^{N_p}$来模拟概率分布的PDF（在粒子滤波部分不再以大写字母标注样本，而是以小写字母带右上角索引角标来标注，仍然有大写加粗的字母为从0时刻累计到当前时刻的所有变量），其中$n$代表了粒子更新的时刻，$i$为各个粒子的索引，$N_s$为所有的粒子数目。它和一般的贝叶斯滤波一样，期望求得的信息是**后验概率**$p(\bold X_n|\bold Y_n)$。根据上一部分IS方法的描述，后验PDF可以表示为：
$$
\hat{p}(x_{0:n}|\bold Y_n)=\sum_{i=1}^{N_p}w_n^i\delta(x_{0:n}-x_{0:n}^i)
$$

于是我们可以选取合适的建议分布$q(x_{0:n})$并从中抽取粒子并计算权重。根据之前的讨论，建议分布应当尽可能的和真实的分布成正比，因此我们应当尽可能的利用所给信息。所以建议分布最好应用上观测数据带来的信息$q(x_{0:n}|\bold Y_n)$。此时粒子的权重为：
$$
w_n^i\propto \frac{p(x_{0:n}^i|\bold Y_n)}{q(x_{0:n}^i|\bold Y_n)}
$$
注意到计算权重需要我们知道后验概率的逐点取值，但这是我们无法求得的，我们只有上一个阶段的后验概率估计，也就是上一阶段的粒子及其权重。**为了计算当前粒子的权重，我们需要利用上一阶段的粒子权重**，而上一时刻的粒子是从分布$q(x_{0:n-1}|\bold Y_n)$中采样得到的，通过一个条件概率可以把上一时刻的建议分布和当前时刻的建议分布联系起来：
$$
q(x_{0:n}|\bold Y_n)=q(x_n|x_{0:n-1},\bold Y_n)q(x_{0:n-1}|\bold Y_n)=q(x_n|x_{0:n-1},\bold Y_n)q(x_{0:n-1}|\bold Y_{n-1})
$$
同理，后验分布也有递推关系：
$$
\begin{aligned}
p(\bold X_n|\bold Y_n)&=p(\bold X_{n-1}|\bold Y _{n-1})\frac{p(x_n|x_{n-1})p(y_n|x_n)}{p(y_n|\bold Y_{n-1})}
\\&\propto p(y_n|x_n)p(x_n|x_{n-1})p(x_{0:n-1}|y_{1:n-1})
\end{aligned}
$$
那么可以得到权重的递推公式：
$$
\begin{aligned}
w_k^i&\propto\frac{p(y_n|x_n^i)p(x_n^i|x_{n-1}^i)p(x_{0:n-1}^i|y_{1:n-1})}{q(x_n^i|x_{0:n-1}^i,\bold Y_n)q(x_{0:n-1}^i|\bold Y_{n-1})}
\\&=w_{k-1}^i\frac{p(y_n|x_n^i)p(x_n^i|x_{n-1}^i)}{q(x_n^i|x_{0:n-1}^i,y_{0:n-1})}
\end{aligned}
$$
于是，**在序贯蒙特卡洛滤波中我们可以调整的便是建议分布$q(x_n|x_{0:n-1}^i,y_{0:n-1})$的选择**，不同的建议分布的选择产生了各种粒子滤波器之间的差异。其中最常用的**SIR粒子滤波器**就采用了最简捷最直观的建议分布：
$$
q(x_n|x_{0:n-1}^i,y_{0:n-1})=p(x_n|x^i_{n-1})
$$
此时有权重更新公式：
$$
w_k^i\propto w_{k-1}^i p(y_n|x_n^i)
$$
通过归一化后得到新的粒子权重$W_k^i$。到目前为止已经完成了贝叶斯框架下的蒙特卡洛序贯滤波，但是这还不足以完成SIR粒子滤波。因为此时的粒子会出现**退化**现象：只有少量粒子占有很大的权重而其余的粒子的权重小到几乎可以忽略。为了解决这一问题需要引入**重采样（Resampling）**：从给出的$N_s$个粒子中按照其其权重的大小为概率有放回的抽取$N_s$次，得到的新的等权重的粒子。这一过程可以看作有权重的**Bootstrap**，因此SIR滤波器也被称为Bootstrap滤波器。综上，SIR粒子滤波的过程可以被概括为：

1. **预测：**根据上一时刻的粒子采样出新的粒子

$$
x_n^i\sim p(x_n|x_{n-1}^i)
$$

这一过程可以由系统的状态方程得到：
$$
x_n^i=f_n(x_n^i, v_n)
$$

2. **更新：**根据新的观测$y_n$更新粒子权重，其中由于重采样上一时刻的粒子权重均为$1/N_s$。可以看到每个粒子的权重是正比于其似然的，这是一个很直观的结果，似然大的粒子拥有更大的权重。

$$
W_k^i=\frac{p(y_n|x_n^i)}{\sum_{i=1}^{N_s}p(y_n|x_n^i)}
$$

3. **重采样：**从已经得到的粒子重采样得到新的等权重粒子并通过新的粒子表示后验PDF。

$$
\{x_n^i, W_k^i\}\xrightarrow{Resampling}\{x_n^j,1/N_s\}
\\p(x_n|\bold Y_n)\approx\frac{1}{N_s}\sum_{j=1}^{N_s}\delta(x_n-x_n^j)
$$

### 集综卡尔曼EnKF 

考虑卡尔曼滤波中的先验和后验协方差矩阵$\bold P_{k|k-1}$和$\bold P_{k}$ ，假如我们知道$k$时刻真实的状态值$x_k^t$。那么就有
$$
\bold P_{k|k-1}=\mathbb{E} [(x_{k|k-1}-x_k^t)(x_{k|k-1}-x_k^t)^T]\\
\bold P_{k}=\mathbb{E} [(x_{k}-x_k^t)(x_{k}-x_k^t)^T]\
$$
但是真实状态是未知的，因此定义集综误差矩阵






