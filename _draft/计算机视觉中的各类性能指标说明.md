# 计算机视觉中的各类性能指标说明

在计算机视觉领域，为了衡量不同模型的优劣，我们需要制定一系列的客观的性能指标。目前经常使用的性能指标都有着较为悠久的统计学历史，对于这一领域的新手来说需要调研许多背景资料才能够掌握。本文旨在在该文篇幅内描述清楚这些指标的来源，定义，及其直观理解，方便读者在阅读其他文献时能够对常见的指标有一个清晰的“感受”。

## 从二分类问题说起

统计学上很重要的一类问题就是假设检验问题，统计学习要解决的二分类问题本质上就是一种假设检验问题。

这类问题需要解决的是：使用恰当的模型，对于给定的样本数据$X$，预测出其类别$y\in\{N, P\}$，即该样本属于阳性（Positive）或者阴性（Negative），或者说是某种假设是否成立。值得注意的是这里采用了Positive和Negative而不是True和False，我认为这是因为Positive和Negative更具普遍性，当我们使用True时一般都会有一个客观的标准在其中吗，而二分类问题往往不会有客观的标准去评价样本是True还是False，更多的是判断样本对应的类别或者说样本对应事件是否发生，也就是假设是否成立。

根据模型的预测结果这里便会出现四种情况，构成了**混淆矩阵**：

|                    | **预测为Positive**   | **预测为Negative**   |
| ------------------ | -------------------- | -------------------- |
| **实际为Positive** | TP（True Positive）  | FN（False Negative） |
| **实际为Negative** | FP（False Positive） | TN（True Negative）  |

虽然Positive和Negative的地位是对偶的，但是我们一般把样本事件发生定义为Positive。比如说在日军在偷袭珍珠港时，雷达用于预测（检测）有没有日军飞行器，那么在这个问题中有日军飞行器按照惯例会被定义为Positive。因此我们更加关心预测为Positive的事件，那么根据TP和FP我们就有了第一对性能指标——TPR（True Positive Rate）和FPR（False Positive Rate）

### TPR和FPR

TPR的定义为所有实际为Positive的样本中，模型也预测为Positive的**比率**：
$$
TPR=\frac{TP}{TP+FN}
$$
FPR的定义为所有实际为Negative的样本中，模型错误预测为Positive的**比率**：
$$
FPR=\frac{FP}{FP+TN}
$$
我们希望模型预测的结果能够**使FPR足够小的同时使TPR尽可能大**。还是以偷袭珍珠港为例，如果TPR太小，那么日军飞机来了没有检测到就会有毁灭性的后果，就如同这个事件的历史事实那样；如果FPR太大，那么模型可能会发生**虚警**，在发动群众避难之后，发现并没有日军飞机前来，那么就会白白浪费掉许多时间和资源。

**那么有没有可能在让FPR减小的同时增大模型的TPR呢？**

答案是不能的，因为这两个量是**耦合**在一起的，当我们减小FPR时，TPR势必会缩小。这一点我们将从这两个统计上的指标的**概率上的counterparts——检测概率$P_D$和虚警概率$P_{FA}$**说明。

### 检测概率、虚警概率与ROC曲线

我们知道监督学习的模型有两种，生成（generative）模型和判别（Discriminative）模型，这两种模型都用来产生条件概率$P(y|X)$，只不过前者直接从训练数据中学习该概率，而后者通过学习$P(X, y)$然后结合先验知识与贝叶斯定理来学习。

![image-20211209190412247](./计算机视觉中的各类性能指标说明.assets/image-20211209190412247.png)

#### 从输出概率的角度理解

#### 从统计量的角度理解

从统计学的角度看，二分类问题是一种假设检验问题。当我们使用某种模型来处理数据的时候，从统计的角度来看，模型输出的结果实际上是一个**统计量**，统计量也是一个随机变量，它也会服从某种分布。

### Accuracy和Precision、Recall与PR曲线

precision is also names user's accuracy. Recall is also known as sensitivity or producer's accuracy.

什么时候需要高recall？当漏检（FN）相对于虚警（FP）更为严重时，比如说对于新冠检测，漏检（感染更多的人）相比较于虚警（隔离并进行治疗）的后果要严重的多。

与之对应的，当虚警（FP）相对于漏检（FN）严重时，比如说对于诈骗电话的检测，虚警（把一个不是诈骗电话的重要电话屏蔽）的后果显然比漏检（放过了一个诈骗电话）带来的损失更大。

相比较于一个曲线，我们更希望使用一个标量指标来描述模型的性能，和ROC曲线的AUC一样，我们也使用PR曲线的AUC作为

### F<sub>1</sub>-Score

F<sub>1</sub>-Score的数学定义时非常简单的，它是precision和recall的调和平均数

I think it is worth mentioning that F1 score is basically e-measure (F_{B}) with B=1. When B>1, more weight is giving to recall, and when B<1, more weight is giving to precision. Setting the value of B is application-dependent. For instance, I've been using B=0.5 to evaluate models for data deduplication to reward models in terms of precision.

### 如何理解ROC曲线和PR曲线中的Baseline

我认为可以从两个角度理解，第一个可以从模型**随机预测（猜测）**样本的角度来看；第二个可以从模型的条件概率分布来看。

## 多分类问题的性能指标

为了把二分类问题中的概念拓展到多分类问题中，我们先从最直观的多分类问题的**NxN混淆矩阵**说起。

OVR（one vs. rest）问题

## 来到目标检测任务中



